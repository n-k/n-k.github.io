## Making a 360 panorama image from UAV images
Spherical Panorama images capture all directions of view from a point. They are typically viewed using specialized viewers which present a section of the full image at a time.

Although there are 360-degree cameras available in the market, most panoramas from UAVs (or drones) are created by taking multiple images from a point from different angles, and then stitching them together with image processing software like blender, hugin, Microsoft ICE etc. The popular computer vision library OpenCV also has a module for stitching and an exmaple for creating spherical panoramas.


This series of posts will describe the process of stitching a panorama from multiple drone images, starting from something very simple, and gradually improving the process.


The process starts with images like the ones below. These have been resized, typically they would be 30-40 megapixels or more. Note the yaw/pitch values, they show that the drone has captured images from many angles. If you are not familiar with the yaw/pitch terminology, think yaw=longitude, pitch=latitude.
```IFRAME:/blog/2020-11-07-1_files/image_info.html
```


This is what the final result looks like. Again, this is resized, typically the outputs would be many 100s of megapixels.
```IFRAME:/blog/2020-11-07-1_files/preview.html
```

The panorama above was made using opencv's stitcher module. This is an overview of what opencv's stitcher does:
![Opencv stitcher](/blog/2020-11-07-1_files/image_stitching_opencv_pipeline.png)

Hmmm. OK, lets start with something simple. Imagine a sphere around the point where the drone is taking images from. We can put each image on a plane normal to the sphere at the point corresponding to (yaw, pitch) of the image. That is, in essence, what I want, wherever I look from the source point, I want to see pixels which the camera captured in that direction. This sounds like it should work...I can use js to prototype it along with the blog. There is a library for extracting metadata from images: https://www.npmjs.com/package/exif . I'll do something hacky to get the yaw/pitch values out of XMP specifically for DJI Phantom 4 Pro, Then use Three.js to render the sphere and images. Sounds simple enough, here's the positioning code:
```javascript
const yaw = parseFloat(xmpAttrs["drone-dji:GimbalYawDegree"]);
const pitch = parseFloat(xmpAttrs["drone-dji:GimbalPitchDegree"]);
const phi = THREE.MathUtils.degToRad(90 - pitch);
const theta = THREE.MathUtils.degToRad(yaw);

const x = radius * Math.sin(phi) * Math.cos(theta);
const y = radius * Math.cos(phi);
const z = radius * Math.sin(phi) * Math.sin(theta);
//...snip...
plane.rotation.z = -roll * Math.PI / 180; // account for roll
plane.position.x = x;
plane.position.y = y;
plane.position.z = z;
plane.lookAt(0, 0, 0);
scene.add(plane);
```

Wait...how do I find the radius of the sphere? That depends on the image sizes and the field of view (FOV) of the camera. The plane rendering an image, should have the same FOV when viewed from the THREE.js scene camera as the drone camera's FOV. If image width is w, sphere radius is r, and horizontal field of view is f, we have `w = 2 * r * tan(f/2)`, or, `r = w / ( 2 * tan(f/2) )`. In this case, f = 73.7 degrees, and w = 256. That makes `r ~= 171`. Indeed, in the widget below, setting radius to about 170 seems to produce good results. There are many simplifying assumptions I have made in the calculations, but its correct in essence.


```IFRAME:/blog/2020-11-07-1_files/mesh.html
```

Notice that images don't line up correctly. There are small shifts in the posiions of objects between neighbouring images. Horizon, lines doesn't line up everywhere etc. A major cause of this is sensor error when measuring yaw/pitch/roll values of the drone.

To account for measurement errors, or even in cases where we don't have the position and orientation data at all, a technique called [Bundle Adjustment](https://en.wikipedia.org/wiki/Bundle_adjustment) (BA) is used. This is a very math heavy technique often requiring GPUs. All the computation allows for not knowing any of the values I have used so far: FOV, position, orientation, sensor size, and focal length (implicitly).

But if I have a sort-of decent starting set of values, is it possible to do something simpler and faster? It should be possible, BA finds the [essential camera matrix](https://en.wikipedia.org/wiki/Essential_matrix) for each image in the set. Essential matrix has many more parameters than what is required in the case of drone imagery. In this case, all the variables except yaw/pitch/roll are known in advance, so for n images, it should be enough to figure out how to find 3n variables, instead of 11n in the case of full BA, or 8n in case of opencv's stitcher, where they assume fixed camera position for spherical stitching. Maybe something iterative capped to a few iterations can be used.

The starting point for these kind of systems is image feature matching. Using algorithms like SIFT, SURF etc. it is possible to find locations of the same point in 2 images. With the simplified assumptions for drone images, let's assume there are N such points where a point is visible in 2 images. Something like:

```javascript
const matches = [
    {
        image1: 0,
        image2: 1,
        position1: {x: 100, y: 100},
        position2: {x, 150, y: 90}
    },
    ...
]
```

And the image information like so:
```javascript
const imageWidth = 256;
const imageHeight = 171;
const horizontalFieldOfView = 73.7 * Math.PI / 180;
const verticalFieldOfView = 53.1 * Math.PI / 180;
const images = [
    {
        yaw: 0.1,
        pitch: 0.1,
        roll: 0.01,
    },
    ...
]
```

These can be used to compare where the matched points should be based on the images' yaw/pitch, and where they actually are in the images. This produces an error function which can be minimized. Note that match data is constant, and image data is variable. This also doesn't try to fix incorrect roll values.

```javascript
function errorForMatch(i) {
    // ignoring roll for now
    const match = matches[i];
    const image1 = images[match.image1];
    const image2 = images[match.image2];
    const predictedYaw1 = image1.yaw + (match.position1.x - imageWidth / 2) * (horizontalFieldOfView / imageWidth);
    const predictedPitch1 = image1.pitch + (match.position1.y - imageHeight / 2) * (verticalFieldOfView / imageHeight);

    const predictedYaw2 = image2.yaw + (match.position2.x - imageWidth / 2) * (horizontalFieldOfView / imageWidth);
    const predictedPitch2 = image2.pitch + (match.position2.y - imageHeight / 2) * (verticalFieldOfView / imageHeight);

    return Math.abs(predictedYaw1 - predictedYaw2) + Math.abs(predictedPitch1 - predictedPitch2);
}

function error() {
    let error = 0;
    for (let i = 0; i < matches.length; i++) {
        error += errorForMatch(i);
    }
    return error;
}
```

Now, a simple gradient descent can be applied. Typically, more appropriate algorithms, like [Levenbergâ€“Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) used by opencv will be used. But let's see where simple gradient descent takes us. My reasoning is that the values are very close to optimal, so a simple approach should work, and not get stuck in a local minima.
```javascript
const delta = 0.001;
const numIterations = 10;
function updateImage(i) {
    const orig = images[i];
    let copy = {...orig};
    copy.yaw -= delta / 2;
    images[i] = copy;
    const error1 = error();
    copy.yaw += delta;
    images[i] = copy;
    const error2 = error();
    images[i] = orig;

    const grad = (error2 - error1) / delta; // d(error)/d(yaw(i))
    images[i].yaw -= grad * delta;

    // ... snip..., do the same for pitch
}

function updateOnce() {
    for (let i = 0; i < images.length; i++) {
        updateImage(i);
    }
}

function update() {
    for (let i = 0; i < numIterations; i++) {
        updateOnce();
    }
}
```

To get the matches, I have a simple python script which uses opencv's SURF and FLANN to get few matches for images pairs and saves them as a JSON file:

```python
#!/usr/bin/env python3
import os
import cv2 as cv
import json


def main():
    dir = os.path.dirname(os.path.realpath(__file__))
    images_dir = os.path.join(dir, 'images')
    image_files = []
    for i in range(0, 50):
        image_files.append(os.path.join(images_dir, f'{i + 1}.jpg'))
    surf = cv.xfeatures2d_SURF.create()
    matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)
    images = []
    key_points = []
    descriptors = []

    for i in image_files:
        image_path = os.path.join(images_dir, i)
        img = cv.imread(cv.samples.findFile(image_path))
        images.append(img)
        kp, des = surf.detectAndCompute(img, None)
        key_points.append(kp)
        descriptors.append(des)

    matches_infos = []
    num_images = len(images)
    for i in range(0, num_images):
        for j in range(0, i):
            try:
                knn_matches = matcher.knnMatch(descriptors[i], descriptors[j], 2)
                ratio_thresh = 0.7
                good_matches = []
                for m,n in knn_matches:
                    if m.distance < ratio_thresh * n.distance:
                        good_matches.append(m)
                # matches = bf.match(descriptors[i], descriptors[j])
                matches = good_matches
                matches = sorted(matches, key=lambda x: x.distance)
                num_matches = min(5, len(matches))
                for idx in range(0, num_matches):
                    mat = matches[idx]
                    img1_idx = mat.queryIdx
                    img2_idx = mat.trainIdx
                    (x1, y1) = key_points[i][img1_idx].pt
                    (x2, y2) = key_points[j][img2_idx].pt
                    matches_infos.append({
                        'image1': i,
                        'image2': j,
                        'position1': {'x': x1, 'y': y1},
                        'position2': {'x': x2, 'y': y2}
                    })
            except:
                pass
    # print(json.dumps(matches_infos, indent = 2))
    with open(os.path.join(dir, 'matches.json'), 'w') as file:
        file.write(json.dumps(matches_infos, indent = 2))


if __name__ == '__main__':
    main()
```

It will be nice to have an interactive widget to run gradient descent and sync the UI. In the widget below, there is a button which starts/stops the gradient descent process.

```IFRAME:/blog/2020-11-07-1_files/mesh2.html
```
